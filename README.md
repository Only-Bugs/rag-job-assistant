# ğŸ§  Job Application RAG Assistant

A **Retrieval-Augmented Generation (RAG)** system that helps you craft tailored **Key skills, cover letters, and email template** for AI/ML roles using your personal project documents and resumes as context.  
Built with **LangChain**, **Ollama (Llama 3.2:3B)**, and **Streamlit** â€” this assistant runs entirely **locally**, no external API needed.

## ğŸš€ Features

- ğŸ“‚ **Document ingestion** â€“ Upload and index your resumes, project summaries, and portfolios
- ğŸ” **Vector-based retrieval** â€“ Context-aware search using **ChromaDB**
- ğŸ§© **LLM-powered reasoning** â€“ Uses **Llama 3.2 (3B)** via Ollama for smart, offline generation
- ğŸ’¬ **Interactive Q&A** â€“ Ask job-specific questions and get personalized answers
- ğŸ–¥ï¸ **Streamlit interface** â€“ Simple UI for interacting with your assistant
- ğŸ“˜ **Jupyter support** â€“ Test, debug, or fine-tune responses directly in notebooks

## ğŸ§° Tech Stack

| Component     | Description                         |
| ------------- | ----------------------------------- |
| **Ollama**    | Local LLM runner (Llama 3.2:3B)     |
| **LangChain** | RAG pipeline & prompt management    |
| **ChromaDB**  | Vector database for semantic search |
| **Streamlit** | Web UI for user interaction         |
| **Python**    | Core logic and orchestration        |
| **Jupyter**   | Experimentation & development       |

## ğŸ“ Directory Structure

```
project_root/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chroma_db/          # Vector database storage
â”‚   â”œâ”€â”€ sample/             # Sample documents
â”‚   â””â”€â”€ job_rag/
â”‚       â””â”€â”€ profile_docs/   # User resumes, project files
â”‚
â”œâ”€â”€ notebooks/              # Jupyter notebooks for testing
â”œâ”€â”€ docs/                   # Architecture / design notes
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ config/         # settings + profile definitions
â”‚   â”‚   â”œâ”€â”€ ingestion/      # loaders, chunking, preprocess, ingest orchestration
â”‚   â”‚   â”œâ”€â”€ vectorstore/    # Chroma client + schemas
â”‚   â”‚   â”œâ”€â”€ models/         # embedding + LLM clients
â”‚   â”‚   â”œâ”€â”€ retrieval/      # retriever + query pipeline
â”‚   â”‚   â”œâ”€â”€ generation/     # prompts + generator workflow
â”‚   â”‚   â”œâ”€â”€ evaluation/     # (future) eval + metrics
â”‚   â”‚   â””â”€â”€ utils/          # text helpers, logging, etc.
â”‚   â””â”€â”€ ui/                 # Streamlit interface (app.py)
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/sheikhmunim/rag-job-assistant.git
cd job_application_rag
```

### 2ï¸âƒ£ Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate   # on Mac/Linux
.\.venv\Scripts\activate   # on Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Install and start Ollama

```bash
# Download Ollama: https://ollama.ai/download
ollama pull llama3.2:3b
ollama serve
```

### 5ï¸âƒ£ Run the app

```bash
streamlit run src/ui/app.py
```

### 6ï¸âƒ£ (Optional) Personalize your profile

Use the in-app **Profile & Role Settings** panel (or edit `data/job_rag/profile_settings.json`) with your contact details, skills, and achievements so the generator can tailor the outputs.

### ğŸ”§ Optional: Runtime overrides

Copy `.env.example` to `.env` and adjust values (e.g., `OLLAMA_HOST`, log level) without touching the codebase:

```bash
cp .env.example .env
# edit .env as needed
```

## ğŸ§© Configuration Files

- `src/rag/config/settings.yaml` â€“ filesystem layout, chunking parameters, and retrieval defaults.
- `src/rag/config/model_config.yaml` â€“ default embedding + LLM model names and Ollama host.
- `data/job_rag/profile_settings.json` â€“ active persona data; edit via code or through the Streamlit **Profile & Role Settings** expander.

## ğŸ§© Environment Variables

| Variable      | Default                  | Description                  |
| ------------- | ------------------------ | ---------------------------- |
| `OLLAMA_HOST` | `http://localhost:11434` | Local Ollama server endpoint |
| `MODEL_NAME`  | `llama3.2:3b`            | Model used for generation    |
| `DATA_DIR`    | `./data/sample`          | Input data directory         |
| `DB_DIR`      | `./data/chroma_db`       | Chroma database path         |

## ğŸ§  Example Queries

- â€œGenerate a cover letter for a Machine Learning Engineer role at Canva.â€
- â€œSummarize my experience with ROS2 and PDDL planning.â€
- â€œWrite a professional email to apply for an AI Engineer internship.â€

## ğŸ§‘â€ğŸ’» Development Notes

- Jupyter notebooks can be used to prototype and test RAG chains.
- Streamlit is used for deployment-ready interactive UI.
- All data stays local â€” **no cloud APIs required**.

## ğŸªª License

This project is released under the **MIT License** â€” free to use and modify with attribution.

## âœ¨ Author

**Sheikh Abdul Munim**
Master of Artificial Intelligence, RMIT University  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/sheikh-abdul-munim-b19391158/)  
ğŸ”— [GitHub](https://github.com/sheikhmunim)
